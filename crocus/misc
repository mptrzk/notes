==================================================
;SECTIONS

sections
todo
hackier
writeup
architecture
filter
crawler
low level crwaling stuff
high level crwaling stuff
scrapy
common crawl
search
uncategorized

==================================================
;TODO

that ranking bug
  why does it give such crappy results?

text wrapping

quickwit index timeout thing

bridge server
  timeout
  fetch error handling

index selection


what's a vector store?

filter load problem

jobdir for carwler
memory leak?

custom ranking

weights
  they don't work!
	https://github.com/quickwit-oss/tantivy/issues/547

proper filter testing
	additional optional column for expected result?

adserver in sources filter

a bit more wait in filter to reduce false positives

symlink on github page?

make the http and peerjs version use the same page layout
  html file can be the same, but with different scripts linked

make the peerjs connection terminate on each search

move status messages up above each result

the fustercluck with the bridge files
  crocus/bridge-clinent, search/static/bridge-server
  yeet the bridge stuff to the "static"
  yeet the static to the top
    name it "front"


ui
  better layout
  status messages
    waiting for the dust to settle
    connecting
    connected <--yeet that in "connectionless" version
    searching
    no resuls
  disabling the search bar when not connected

making it render properly on mobile

index selection dropdown list

try nojs index

multiple pages

about page
  inserting images nicely

==================================================
;HACKIER

glue up as much as possible

==================================================
;WRITEUP


  Crocus is a search engine that tries to not index pages with ads.
  Crocus is a search engine that tries to not return pages with ads.
  Crocus is a search engine that tries to not show pages with ads.
  Crocus is a search engine whose results don't contain ads.

  Crocus is a search engine that doesn't show pages that try to show ads
  Crocus is a search engine that doesn't show pages that try to display ads


Crocus is a search engine that tries to not index crappy websites,
Which I think mostly comes down to not indexing websites that make money by showing people ads,

Which I think mostly comes down to not indexing pages with ads.



It's basically adblock squared. With adblock you don't see the ads.
With cocus you don't see pages which try to show the ads.
You want that, because presence of ads correlates with low quality content that's
purposfully written to direct your attention to ads.

Right now it runs on my laptop but can be accessed through a github.io page thanks to PeerJS 
I'll host it properly when I'll be able to afford it.
It needs scale to get really good.

How it works (right now):

The main heuristic for now is that:
"If a homepage of a website doesn't contain ads, it's unlikely that other pages on that site do"

the heuristc
  Why?
    my laptop slow
    my disk small
    internet big
    very very big

right now it just does keyword search using quickwit
eventually I want to have some weird language model conversational shit, where I can say "naah, what I acually meant was this and that"

Common crawl list of domains
filter homepages
crawl websites



Testing pages for ads
open pages with a headless browser and test them for ads using [library]

crawling using scrapy

It seems to me that a skilled person could do the whole thing better in like 2 days.

business model:
servers need food
people do too
open source everything
make the unpaid users wait a little bit


TODO:
  pipelining
  semantic search
  scaling
  money-making
  


Crawling

Indexing & search
Right now it uses Quickwit.
I want 


Why "Crocus"


So I took up what I call "subtractive gardening"

After I pulled all the plants I thought I had enough out there somewhere, the couch grass started popping everywhere.
It's really weird when you accidentally summon the genetic algorithm against you...

Later that night

50000 crocuses
	weirdness without hype
		not that hype is necessarily bad

all the japanese liverworts

I thought "This kicks ass! If I were to google liverworts nowadays, I'd get "we value your privacy" popup and 
then an article... oh wait, a "newsletter popup"... umm, an "article" copywritten by some poor soul who isn't really into gardening;
who had to stretch 4 liverworthy sentences into a 3 page litany to be then diluted 50/50 by ads and "suggested articles"; and also to hit all
the SEO keywords or something. 

10 Useless ways to do something stupid

The old web was soo cool
Even complete balderdash was written by people who care
It was all so information dense
You could tell that any writing was edited down, rather than stretched
people there had more IQ
but also there was no money in ads 
The internet users were not a considerable market for almost anything
The next day I thought about what makes the today's web crappy
I thought... Popups... Ads... Newsletters... Crapy writing... ads!
Maybe it all stems from ads
and ads are the easiest to filter out

So first I wrote a filter for google search...
First I wrote a python script that looked for URLs pointing to known ad servers.
It worked quite poorly. Interestingly enough, it worked better when googling for
Polish sites.
Then I did the headless browser thingy I mentioned earlier, but only waited for ads
until DOMContentLoaded event fired.
I still got about 50% false positives.




If you've made something better, email me!


what is the web for
  getting information
  does JS improve you getting the information

if you're looking for sites to be read and navigated, you don't need js


spartan nojs
amish?

most js is garbage or provides very little value

no ads before dom load + no deferred js
	checking for deferred js
		puppeteer?


is pagerank pwnable?
	the question is how is it pwnable

;business stuff

mismatch between value to the end user and the value to the contractor
  speak normally, please

newsletters to ask for email
SEO spam


google papers


computers and programmers need food
we sell the part of it that really scales
  ml models, indexing,

how google really makes money through search?
  the "ad" checkbox?
    does my adblock block it?
      doesn't seem like so

gugl
	freeloaders
implicit stochastic pimping

one of those ad boards

==================================================
;ARCHITECTURE

scaling
  kafka?

ranking n all when stuff's bigger than it would fit in ram
  use some persistant storage

read:
https://realpython.com/python-async-features/

crawler-first arch?
  calls filter up to a threshold in a given FLD
  making "parse" for multiple requests wait until filtering finishes
    list of promises to resolve for each domain (until threshold is passed)
      or rather list of lambdas that resolve promises when called

interface between systems
  filter writes to the domains file
  crawler writes to crawled domains file
  cron job sends the diff of both files to the crawler to be crawled
  same can be done for indexer ingestion

or maybe it can be done as a single pipe
  but it should be done in a way where I don't have to restart everything

maybe write both to a pipe and a file
when restarting, the file can be yeeted to a pipe
^^ do it with files for now

making use of cc metrics


the problem with resource allocation


the systems
  filter
  crawler
  indexer, search

named pipes
  are they good?

homepage-first ad-free heuristic
  but do it for hosts, if they are what I think they are
    check it out!
filter home pages with puppeteer
scrape clean websites with scrapy

filter-crawler interface
  generator that waits for a signal?

==================================================
;FILTER

pages can be scanned for just requests

	
filter load problem
	open tabs with legit pages shouldn't take much cpu/bandwidth
	but some pages can run a boatload of javascript
	concurrency should be adjusted to cpu load

how much do empty pages lag?
how much 

proper logic
  blocker promise races with all others
  the others are
    dcl
    timeout
  timeout promise gets dispatched after dcl
  timeout races with dcl
  and then there's the wait

give some sites a second chance

also, some additional delay after domcontentloaded

handle signals for graceful exit
  letting tasks finish for one ^C
  exit with two ^C's 

tainted batch recycling
  doesn't seem important
  maybe it is if critical bugs happen too often

command line args
  --reset option

the weird bug
  why is it around 1140?
    seems like a coincidence
      it happens if I start crawling later
when does it throw exceptions and when it doesn't
it looks like a bug in puppeteer
  the bug still happens if I restart browser each time
  maybe it's the bug in blocker
what is the computer doing when it freezes?
  how to check that for arbitrary program?
difference between bug from "hosts" and bug from "domains"

why one worker gets frozen on page 1136?
why when there are 2 pages, it doesn't freeze, but doesn't crawl the page either?
why is one worker available before it finishes crawling?

is there a way to print all the exceptions? even those which get caught?

workarounds
  restart the cluster
    isn't it the same as restarting browser
      bet it ain't gonna work, but there's one way to find out
  restart the process

a promise which resolves when the cluster completes a task
  isn't it analogoust to that idea I had for crawler-first approach?
can I resolve a promise from outside?
an easier way would be to use an absolute timer and compare

there can be a whole procedure for dealing with shitty bugs
  the simplest way would be to yeet the whole batch into a file
  it would be useful If I pulled the indices from the file as well
  I wouldn't have to count reads, which is a bit messy

rationale for separate "blockers" for concurrency

fork puppeteer-cluster for worker-specific context?

or just create an array of as many blockers as concurrent workers and bind
them in task functions
can I just cycle the blockers, or do I have to check if they are bound?
  you have to check:
    worker 1 starts
    worker 2 starts
    worker 2 finishes
    worker 2 starts <-- here it would get the same blocker as worker 1

workers have IDs, you can use them to assign blockers
    


the abstraction I'm looking for
  a promise queue that automagically gets filled up to n promises using a generator
    async pool

concurrent tabs
  resolve an array of promises concurrently
  is there something like that for generators?

consent-o-matic

installing extensions for puppeteer

making it distributed

take the commoncrawl host site links
  [url]
  why not use the alexa 1 million?
    is it up to date
    how is it made

how about crawling the hosts?
  are github.io pages hosts?
    prolly
      check commoncrawl

setting $DISPLAY

weird ID bug
  apparently not opening each url in new page helps
gr

false negatives
  paypal.com
  "
    facebook.com
    instagram.com
  "
    using just one page helps
      one blocker per page?
        it uses an external file, is it ok to instantiate it?
        one way to find out!
        also sniff when it I/Os
  en.wikipedia.org
    disabling and re-enabling blocker after each page helped
  "
    archive.org TODO
    github.com
  "
    one blocker per ?page sequence?
    
not closing pages at all

not awaiting page close helps with the weird error

install the same adblock, use the same list
use probe
check if preceding site makes a difference

false positives
  bbc.co.uk
    there's a delayed redirect

ProtocolError: Network.setCacheDisabled timed out. Increase the 'protocolTimeout' setting in launch/connect calls for a higher timeout if needed.

==================================================
;CRAWLER

JSON doesn't like bytes, lxml doesn't like utf8

chnk
  2:38

maybe quickwit couldn't ingest the chnk data because of missing fields?

GIL
  multiprocessing?
  yaaaaaa?

and tqdm can be wrapped around the line generator, not the thingimabogler

measure throughput with and without separate sanitizer
try to make the thread workers work
or maybe just use regular threads...

the CHUNKY way
  do things in large chunks
  avoid context switches
but wasn't my crawler really easy on the IO?
  If it was, It doesn't mean that the IO was easy on the crawler

retry nojs crawl with redirects disabled
 
allowed domains instead of same-domain links?
	supposedly it's problematic
		one way to find out


can I put "yield" in a lambda and pass it to external function?
  no

better text cleanup
  no css
check out how I did it before

testing
  crawler-test.com
  
setting request priority

prioritizing homepages when crawling?
  scrapy depth priority


why doesn't it quit
  is that while loop necessary?

making scrapy operate a UNIX named pipe
  there's a pipe
  another process puts stuffs there

the crawler shouldn't quit when no crawls are scheduled
derviative requests should proceed normally

crawler vs spider in scrapy

multiple spiders?

==================================================
;LOW LEVEL CRWALING STUFF

problems
  high load time pages
    examples:
      high latency pages
      low bandwidth pages
      huge pages
    can I emulate them?
  how not to get blocked?
^^ scrapy has download timeouts
  read up on how they are implemented

redirects

==================================================
;HIGH LEVEL CRWALING STUFF

correct header - what about xml, xhmtl & stuff?
uncrawlables can be identified by extension as well

mime type and detected mime type
mime type in response headers?

==================================================
;SCRAPY

"you don't need scrapy for JS, because it can do requests concurrently already"
  seems really fishy, but why?
    stuff like number of concurrent requests
    scheduling
    robots.txt
    duplicates

vk detecting that I'm not using chrome
  supposedly html headers other than UA leak information about the browser
    copied headers, still doesn't work

are the headers really identical
  compare requests from browser and scrapy with wireshark or something

making it run in the bg

blacklist with scrapy?
  just a normal set, like in the old crawler
  there's some blacklist stuff for distributed scrapy on the web

IgnoreRequest
StopDownload
^^what did I want that for?

RTFM (all of it!)

about 30000 pages over night
  it's much worse tham my previous crawler
^^?clobbered? by slow requests?


why it didn't have the header problem in the "parse" method?
  what header problem?
    i was trying to yeet requests to documents with wrong file types

can the DNS server be throttling me?
twistedIO threadpool
what is twisted anyway?
how many concurrent requests is enough
is there a way to measure my current total download bandwidth
what's the exact message?

telnet console
  does it work on the other computer?

manually setting request priority 
does scrapy schedule non-start requests when start_requests didn't finish?
  how to test that?

==================================================
;COMMON CRAWL

common crawl
  first, I just want page urls and their links
  or maybe just download subsets of data,
  yeet stuff with scripts, download more, etc
    tried, bad idea

  is offset in bytes or lines?
    what offset?

==================================================
;QUICKWIT

http://127.0.0.1:7280/ui/search?query=severity:ERROR&index_id=vector-otel-logs&max_hits=10 
	^^something something ui, check this out

columnar, document and index store
effective cardinality

fields:
  record
  tokenizer
  stored
  fast
  indexed
  fieldnorms

/quickwit/quickwit-serve/src/search_api/rest_handler.rs
	120 - struct definition
	192 - 
  204 - root_search
	224

==================================================
;SEARCH

better backend?

meilisearch?

small-scale search index

that quickwit stuff might be too sophisticated
maybe try elasticlunrjs or smth

quickwit

==================================================
;FRONTEND

==================================================
;UNCATEGORIZED

reorganize the fustercluck

\    _    /
|\__/ \__/|
| _|+_+|_ |
|/ \_ _/ \|
/   | |   \
   
