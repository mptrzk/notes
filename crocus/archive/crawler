==================================================
;SECTIONS

sections
todo
far out
crawl heuristics
incremental save
bugs
refactor
uncategorized


==================================================
;TODO


blacklisting heat logic
clean up

==================================================
;FAR OUT

updating index/page repo
multiple crawlers

ml
  embedding
  optimizers

0 downtime [move appropriately]
  continuous integration?
  run in containers & switch ports?


==================================================
;CRAWL HEURISTICS

[hindsight - meh]

ad stink

can the script request an ad?
  any js
  request
  third-party url in script as a string constant

boredom
  same parent for too long
  domain based?

heat
	tracked for each parent and domain
	each loop step is counted


==================================================
;INCREMENTAL SAVE

not just appending
  stuff that's in old domain list might be blacklisted
  heat gets updated every step

queue can be regenerated

sites as ordered dict?
  no queue
  just find the oldest uncrawled site

just key lists in class

but blacklist is a set


==================================================
;BUGS

duplicate crawling
  differnet protocols?
    it's not a bug!
      or is it?
      sometimes https sites got blocked while http didn't
        maybe it's the crawl order:
          http://foo.com/bar - pass
          http://foo.com/ads - block
          https://foo.com/bar - block

http://ctm.crouchingtigerhiddenfruitbat.org/pub/cygwin/circa/2019/02/21/161443/index.html
	large site with consistent, slow download

pages like this - https://www.rpmfind.net/linux/RPM/CentOS.html 
	too many links
	links to non-html stuff?
    not on that particlular page
requesting just content info
  deeper dive into http

https://episcopalchurch.org/files/ellibrodeoracioncomun_0.pdf
	doesn't detect wrong filetype

sites with iframes?
	https://www.cartoongamez.com/game10.html

sneaky bug
  site instead of link
  
==================================================
;REFACTOR

[hindsight - naah]

informal description
A crawler visits sites, discovers links for other sites and visits them
  errors
  blacklist
  saving
    restart

defaultdicts and dataclasses considered harmful?
  can't have nice things without introducing even more complexity
  defaultdicts
    can't initialize default from key
    dataclasses 
      mutable init

everything by key?
  nah?
    pointers are faster
    doing it with keys is messier

storing dict keys in dict entries?
  yay
    unnecessary
  nay
    paragraph above
    pleasing?
      rly?


nondeterministic part - request

separating nondeterministic part?


true functions

pages are dataclasses


"fetch" downloads the page and sets its status
  a method of Page

"crawl":
  - checks blacklist
  - tries to fetch the page
  - looks for links
  - adds an inlink to each linked page:
      linked pages are added to the repo if they don't exist
      ...
    ^^ meh, a lot of messing around with stuff that doesn't come to mind when you think "crawl"
      but that's simple stuff

per-page crawl function
  it can be done asynchronously

storing blacklisted page objects?
    
but fetching and crawling separately means passing outlinks
  what's wrong with that?
          

page never exists on its own
  page repo?


site doesn't push itself
  push_site does

domain dict
  heat
  blocked?

domain as
  dict?
  dataclass?
  class?

==================================================
;TESTING

crawler-test.com

==================================================
;UNCATEGORIZED


scrapy?

deduplication
  hash the html
    hash as the key?
  strip stuff after '#' in urls

"known" storing urls or sites?
  does it matter?
  it's just a pointer

hasad server

rank heuristics and crawl heuristics


"Site" as class
  storing parents and children?

heuristics updated while site in queue?

dead domains 



storing offline html?

async?

ordered dict and infinite iteration

